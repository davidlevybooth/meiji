Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	25	bbduk
	26

[Sat Feb  1 13:47:02 2020]
rule bbduk:
    input: /home/data/ssmstest/reads/101-FEC-1-8-S4.fastq
    output: /home/david/MEIJI/reads/101-FEC-1-8-S4.fastq.q30.fq
    jobid: 20
    wildcards: sample=101-FEC-1-8-S4.fastq
    threads: 8

Terminating processes on user request, this might take some time.
[Sat Feb  1 13:47:03 2020]
Error in rule bbduk:
    jobid: 20
    output: /home/david/MEIJI/reads/101-FEC-1-8-S4.fastq.q30.fq

RuleException:
CalledProcessError in line 72 of /home/david/MEIJI/0.1/rules/prepare_reads.smk:
Command ' set -euo pipefail;  
        bbduk.sh -Xmx20g t=8 in=/home/data/ssmstest/reads/101-FEC-1-8-S4.fastq out=/home/david/MEIJI/reads/101-FEC-1-8-S4.fastq.q30.fq qtrim=r trimq=30 minlen=130 ftl=10 ftr=160 ' returned non-zero exit status 130.
  File "/home/david/MEIJI/0.1/rules/prepare_reads.smk", line 72, in __rule_bbduk
  File "/home/david/miniconda3/envs/Praxis/lib/python3.6/concurrent/futures/thread.py", line 56, in run
Removing output files of failed job bbduk since they might be corrupted:
/home/david/MEIJI/reads/101-FEC-1-8-S4.fastq.q30.fq
Complete log: /home/david/MEIJI/0.1/.snakemake/log/2020-02-01T134702.430929.snakemake.log
